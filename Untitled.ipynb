{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_kaggle__data():#clean the data get from Kaggle\n",
    "    df = pd.read_csv('./kaggle_.csv')\n",
    "    item_lis = []\n",
    "    for i in range(len(df)):\n",
    "        tmp = set()\n",
    "        for k in df.iloc[i]:\n",
    "            strr = k.split(',')\n",
    "        for s in strr:\n",
    "            tmp.add(s)\n",
    "        item_lis.append(tmp)\n",
    "    return item_lis\n",
    "\n",
    "def clean_data(): #clean the data generate by IBM #data_10^4trans\n",
    "    #print(item_set)# The list of set of each transaction\n",
    "    #print(len(item_set))# The number of all transaction\n",
    "    df = pd.read_csv('./AR.csv')\n",
    "    df['transaction'] = \"\"\n",
    "    df['item'] = \"\"\n",
    "    df.rename(columns={\"Col\":\"tmp\"}, inplace = True)\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        tmp = df[\"tmp\"][i].split()\n",
    "        df['transaction'][i] = tmp[1]\n",
    "        df['item'][i] = tmp[2]\n",
    "\n",
    "    df.drop([\"tmp\"], inplace = True, axis = 1)\n",
    "\n",
    "    items = df.groupby(\"transaction\")  #print(items.groups) #0-962 kinds of items appear in which transaction\n",
    "    item_dict = items.groups\n",
    "    item_set = []\n",
    "\n",
    "    for key in item_dict:\n",
    "        tmp = set()\n",
    "        for i in list(item_dict[key]): # The item exist in the transaction\n",
    "            item = df['item'][i]\n",
    "            tmp.add(item) # The list of item of #key transaction\n",
    "        item_set.append(tmp)\n",
    "\n",
    "    return item_set\n",
    "i_tran_lis = clean_data()\n",
    "tran_lis = clean_kaggle__data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_set(set_, trans_num, min_sup):#This function is going to get the set that fit the min_sup\n",
    "    l = 1\n",
    "    set_dict = {}\n",
    "    freq_dict = {}\n",
    "    for set__ in set_: #list of set\n",
    "        for ele in combinations(set__, l): \n",
    "            if len(ele) !=0:\n",
    "                ele_ = set((ele))\n",
    "                if (str(ele_ ) in set_dict):\n",
    "                    set_dict[str(ele_ )]  = (set_dict[str(ele_ )][0], set_dict[str(ele_ )][1]+1)\n",
    "                    freq_dict[str(ele_ )]  = (freq_dict[str(ele_ )][0], freq_dict[str(ele_ )][1]+1)\n",
    "                else:\n",
    "                    set_dict[str(ele_ )] = (ele_, 1)\n",
    "                    freq_dict[str(ele_ )] = (ele_, 1)\n",
    "            else: break\n",
    "    l += 1\n",
    "    set_dict = check_fit_min_sup(set_dict, min_sup, trans_num)\n",
    "    freq_dict = check_fit_min_sup(freq_dict, min_sup, trans_num)\n",
    "    while True:\n",
    "        _set = set()\n",
    "        for key in set_dict:\n",
    "            for e in set_dict[key][0]:\n",
    "                _set.add(e)\n",
    "        set_dict = {}\n",
    "        tmp_set = list(combinations(_set, l))\n",
    "\n",
    "        for e in range(len(tmp_set)):\n",
    "            s = set()\n",
    "            for k in range(len(tmp_set[e])):\n",
    "                s.add(tmp_set[e][k])\n",
    "            tmp_set[e] = s\n",
    "       \n",
    "        for i in range(trans_num):\n",
    "            for s in tmp_set:\n",
    "                if(s & set_[i] == s):\n",
    "                    if (str(s) in set_dict):\n",
    "                        set_dict[str(s)]  = (set_dict[str(s)][0], set_dict[str(s)][1]+1)\n",
    "                        freq_dict[str(s)]  = (freq_dict[str(s )][0], freq_dict[str(s)][1]+1)\n",
    "                    else:\n",
    "                        set_dict[str(s)] = (s, 1)\n",
    "                        freq_dict[str(s)] = (s, 1)\n",
    "        ans_dict = check_fit_min_sup(set_dict, min_sup, trans_num)\n",
    "        freq_dict = check_fit_min_sup(freq_dict, min_sup, trans_num)\n",
    "        if (len(ans_dict)==0):\n",
    "            return freq_dict\n",
    "        l +=1\n",
    "    \n",
    "def check_fit_min_sup(dict_, min_sup, trans_num):\n",
    "    re_dict={}\n",
    "    for key in (dict_):\n",
    "        if(float(dict_[key][1]/trans_num) >= min_sup):\n",
    "            re_dict[key] = dict_[key]\n",
    "    return re_dict\n",
    "def fp_get_rule_with_conf(freq_set, min_conf):\n",
    "    #for every freq set :\n",
    "    #   for every possible set in each freq set without the set itself\n",
    "    #          the possbilty = (the times / trans_num) of freq set / the times/ trans_num of the possible set\n",
    "    #           if the possbilty > min_conf:\n",
    "    #               rule get\n",
    "    s = set()\n",
    "    subtra = set()\n",
    "    tt = []\n",
    "    for fs in freq_set: #for all freq set\n",
    "        #print(fs)\n",
    "        if len(freq_set[fs][0]) > 1 : #This set is more than one ele\n",
    "            for l in range(1, len(freq_set[fs][0])): \n",
    "                tmp_set = combinations(freq_set[fs][0], l)#all possible set of that set #{a, b}, {a}, {b}\n",
    "                for i in tmp_set:\n",
    "                    for k in range(len(i)):\n",
    "                        s.add(i[k])\n",
    "                    if len(s) >= len(freq_set[fs][0]):\n",
    "                        subtra = s - freq_set[fs][0]\n",
    "                    else:\n",
    "                        subtra = freq_set[fs][0] - s \n",
    "\n",
    "                    for key in freq_set: #分子是母set的出現次數, 分母是該set的出現次數\n",
    "                        if freq_set[key][0] == subtra:\n",
    "                            #print( \"s:\", s, \"sub:\", subtra,\"sub_num:\", freq_set[key][1], \"freq_set:\",  freq_set[fs][0], \"freq_num: \", freq_set[fs][1])\n",
    "                            pos = float(freq_set[fs][1] / freq_set[key][1])\n",
    "                    if (pos >= min_conf):\n",
    "                        st = str(s) + \"->\" + str(subtra)\n",
    "                        tt.append(st)\n",
    "                    s = set()\n",
    "                    subtra = set()\n",
    "    ans = sorted(tt)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def get_rule_with_conf(freq_set, min_conf, trans_num):\n",
    "    #for every freq set :\n",
    "    #   for every possible set in each freq set without the set itself\n",
    "    #          the possbilty = (the times / trans_num) of freq set / the times/ trans_num of the possible set\n",
    "    #           if the possbilty > min_conf:\n",
    "    #               rule get\n",
    "    s = set()\n",
    "    subtra = set()\n",
    "    tt = []\n",
    "    for fs in freq_set: #for all freq set\n",
    "        #print(fs)\n",
    "        if len(freq_set[fs][0]) > 1 :\n",
    "            for l in range(1, len(freq_set[fs][0])): \n",
    "                tmp_set = combinations(freq_set[fs][0], l)#all possible set of that set\n",
    "                for i in tmp_set:\n",
    "                    for k in range(len(i)):\n",
    "                        s.add(i[k])\n",
    "                    if len(s) >= len(freq_set[fs][0]):\n",
    "                        subtra = s - freq_set[fs][0]\n",
    "                    else:\n",
    "                        subtra = freq_set[fs][0] - s\n",
    "                    \n",
    "                    for key in freq_set:\n",
    "                        if len(freq_set[key][0] - subtra) == 0 :\n",
    "                            #print( \"s:\", s, \"sub:\", subtra,\"sub_num:\", float(freq_set[key][1]/trans_num), \"freq_set:\",  freq_set[fs][0], \"freq_num: \", float(freq_set[fs][1]/trans_num))\n",
    "                            pos = float((freq_set[fs][1]/trans_num) / (freq_set[key][1]/trans_num))\n",
    "                            #print(pos)\n",
    "                    \n",
    "                    if (pos >= min_conf):\n",
    "                        st = str(s) + \"->\" + str(subtra)\n",
    "                        tt.append(st)\n",
    "                    s = set()\n",
    "                    subtra = set()\n",
    "    ans = sorted(tt)\n",
    "    return ans\n",
    "    \n",
    "def get_sup(dict_, sup, trans_num):\n",
    "    for key in dict_:\n",
    "        print(dict_[key][0], \" : \", float(dict_[key][1]/trans_num))\n",
    "def fp_get_sup(dict_):\n",
    "    for key in dict_:\n",
    "        print(dict_[key][0], \" : \", float(dict_[key][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ele_in_seq(tran_lis, trans_num, min_sup):\n",
    "    re_dict = {}\n",
    "    ans_dict = {}\n",
    "    for ele_ in tran_lis:\n",
    "        for ele in ele_:\n",
    "            if (str(ele) in re_dict):\n",
    "                re_dict[str(ele)]  = (re_dict[str(ele)]+1)\n",
    "            else:\n",
    "                re_dict[str(ele)] = (1)\n",
    "    for key in re_dict:\n",
    "        if(float(re_dict[key]/trans_num)>= min_sup):\n",
    "            ans_dict[key] = re_dict[key]\n",
    "    return ans_dict\n",
    "def ele_in_trans_in_seq(tran_lis, feq_dict):\n",
    "    trans_in_seq = []\n",
    "    tmp_ = []\n",
    "    sor_dict = []\n",
    "    tmp_dict = {}\n",
    "    for tran in tran_lis: #type(tran) = set\n",
    "        for e in tran:\n",
    "            for set_ in feq_dict:\n",
    "                if(e == set_):\n",
    "                    tmp_dict[str(e)] = feq_dict[set_] #出現次數\n",
    "                    break\n",
    "        sor_dict.append(tmp_dict)\n",
    "        tmp_dict = {}\n",
    "    for set_ in sor_dict:\n",
    "        r = sorted(sorted(set_.items(), key = lambda i : i) ,key = lambda k : k[1], reverse = True)\n",
    "        for i in r:\n",
    "            tmp_.append(i[0])\n",
    "        trans_in_seq.append(tmp_)\n",
    "        tmp_ = []\n",
    "\n",
    "    return trans_in_seq\n",
    "def build_cond_tree(paths):\n",
    "    cond_tree = FPTree()\n",
    "    condition_item = None\n",
    "    items = set()\n",
    "\n",
    "    for path in paths: #{B, MA, MI, J}, {B, TE, MA, J}\n",
    "        if condition_item is None:\n",
    "            condition_item = path[-1]._item\n",
    "        #con = JAM\n",
    "        point = cond_tree.get_root()\n",
    "        for node in path:\n",
    "            next_point = point.search(node._item) #check this node's children contains this item or not\n",
    "            if next_point == None:\n",
    "                items.add(node._item)\n",
    "                count = node.get_num() if node._item == condition_item else 0\n",
    "                next_point = FPNode(cond_tree, node._item, count)\n",
    "                point.add(next_point)\n",
    "                cond_tree.update_route(next_point)# Tree add B\n",
    "            point = next_point\n",
    "\n",
    "    assert condition_item is not None\n",
    "\n",
    "    # Calculate the counts of the non-leaf nodes.\n",
    "    for path in cond_tree.get_all_path(condition_item): #B, MA, MI\n",
    "        count = path[-1].get_num() # 1\n",
    "        for node in reversed(path[:-1]):\n",
    "            node._num += count #+=1\n",
    "\n",
    "    return cond_tree\n",
    "def find_with_suffix(tree, suffix, trans_num): #suffix 後綴\n",
    "        for item in tree.items():#for all item in the tree\n",
    "            total_sup = float(tree.get_total_sup(item)/trans_num)#item support\n",
    "            if total_sup >= min_sup and item not in suffix:\n",
    "                found_set = [item] + suffix #Beer\n",
    "                yield (found_set, total_sup)\n",
    "                #(JAM, 2), ([Beer, cold], 2)\n",
    "                # Build a conditional tree and recursively search for frequent\n",
    "                # itemsets within it.\n",
    "                cond_tree = build_cond_tree(tree.get_all_path(item))\n",
    "                for s in find_with_suffix(cond_tree, found_set, trans_num):\n",
    "                    yield s # pass along the good news to our caller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPTree(object):\n",
    "\n",
    "    route = namedtuple('Route', 'head tail')\n",
    "\n",
    "    def __init__(self):\n",
    "        self._root = FPNode(self, None, None)#The root of fptree is null\n",
    "        self._route = {}\n",
    "\n",
    "    def items(self):\n",
    "        item_lis = {}\n",
    "        for r in self._route:\n",
    "            item_lis[r] = self._route[r]\n",
    "        return item_lis\n",
    "\n",
    "    def add_trans(self, trans): #Add node into tree\n",
    "        init_point = self.get_root() # point to the root\n",
    "        for item in trans:\n",
    "            next_point = init_point.search(item) #Ensure that node contains a child node of that item or not\n",
    "            if next_point == None: #If it is a new item \n",
    "                next_point = FPNode(self, item) #Init a new node\n",
    "                init_point.add(next_point) #This node become a child\n",
    "                self.update_route(next_point) #The route add a new node, so it has to be updated\n",
    "            else:\n",
    "                next_point.increase_num()\n",
    "            init_point = next_point\n",
    "\n",
    "    def update_route(self, node): #Since it is a new node go into the tree, so the route of the item must be updated \n",
    "        try: #If there exist a route that contains the item of the node, then the tail of the route will be that item and it's neighbor will be it too\n",
    "            present_route = self._route[node._item]\n",
    "            present_route[1]._neighbor = node\n",
    "            self._route[node._item] = self.route(present_route[0], node)\n",
    "        except KeyError: #New node so the route does not exist and that item will be the head of the route\n",
    "            self._route[node._item] = self.route(node, node) #The node will be the head and the tail of the route\n",
    "\n",
    "    def get_all_path(self, item):\n",
    "        #print('head', self._route[item][0]._item, self._route[item][0], self._route[item][0].get_num() )\n",
    "        head = self._route[item][0]\n",
    "        total_list = []\n",
    "        while True:\n",
    "            if(head != None):\n",
    "                p = self.get_prefix_path(head)\n",
    "                total_list.append(p)\n",
    "                head = head.get_neighbor()\n",
    "            else:\n",
    "                return total_list\n",
    "\n",
    "    def get_prefix_path(self, node): #Get the prefix path of given node\n",
    "        path = []\n",
    "        path.append(node)\n",
    "        node = node.get_parent()\n",
    "        while node != self.get_root() and node != None:\n",
    "            path.append(node)\n",
    "            node = node.get_parent()\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    def get_total_sup(self, item_name):\n",
    "        total = 0\n",
    "        item_head = self._route[item_name][0]\n",
    "        while True:\n",
    "            if (item_head != None):\n",
    "                total += item_head.get_num()\n",
    "                item_head = item_head.get_neighbor()\n",
    "            else: break\n",
    "        return total\n",
    "\n",
    "    def get_root(self):\n",
    "        return self._root\n",
    "class FPNode(object):\n",
    "    def __init__(self, tree, item, num = 1):\n",
    "        self._tree = tree\n",
    "        self._item = item #The item put in the node\n",
    "        self._num = num #The number of item appears in the whole transaction\n",
    "        self._children = {} #\n",
    "        self._neighbor = None\n",
    "        self._parent = None\n",
    "    \n",
    "    def add(self, child): #Add child into node\n",
    "        if not child._item in self._children:\n",
    "            self._children[child._item] = child\n",
    "            child._parent = self\n",
    "\n",
    "    def search(self, item):#Search the item exist or not\n",
    "        try:\n",
    "            return self._children[item] \n",
    "        except KeyError:\n",
    "            return None\n",
    "    \n",
    "    def get_num(self): #Return the number of this item appeaers\n",
    "        if self == None:\n",
    "            return 0 \n",
    "        else:\n",
    "            return self._num\n",
    "\n",
    "    def get_neighbor(self): #Return the neighbor of the node -> neighbor is the same item \n",
    "        return self._neighbor\n",
    "\n",
    "    def get_parent(self):\n",
    "        return self._parent\n",
    "\n",
    "    def increase_num(self): #Increase the count of the item appears\n",
    "        self._num += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0020160675048828125\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.04388999938964844\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0020003318786621094\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.569312334060669\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.005051136016845703\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.042168378829956055\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0020313262939453125\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5995914936065674\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.03948402404785156\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.010232925415039062\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5752172470092773\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0027954578399658203\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.05210995674133301\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.6045877933502197\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.05540657043457031\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5821444988250732\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0021445751190185547\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.03764224052429199\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5901713371276855\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.03978919982910156\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5974133014678955\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.0408320426940918\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.6221225261688232\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.03969597816467285\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.580399751663208\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "FPG_kaggle_dataset_runtime:  0.0\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "FPG_IBM_dataset_runtime:  0.034725189208984375\n",
      "-----------------------------------------------------------------\n",
      "Start_Kaggle\n",
      "Apriori_kaggle_dataset_runtime:  8.893013000488281e-05\n",
      "-----------------------------------------------------------------\n",
      "Start_IBM\n",
      "Apriori_IBM_dataset_runtime:  0.5897243022918701\n",
      "-----------------------------------------------------------------\n",
      "fk 0.001200723648071289 fi 0.04257435798645019 ak 0.0014353513717651368 ai 0.5910684585571289\n"
     ]
    }
   ],
   "source": [
    "min_sup = 0.2\n",
    "min_conf = 0.4\n",
    "f_k_t = 0\n",
    "f_i_t = 0\n",
    "a_k_t = 0\n",
    "a_i_t = 0\n",
    "for i in range(10):\n",
    "    '''tran_lis = clean_data()\n",
    "    i_tran_lis = clean_kaggle__data()'''\n",
    "    print(\"Start_Kaggle\")\n",
    "    start = time.time()\n",
    "    feq_dict = ele_in_seq(tran_lis, len(tran_lis), min_sup)\n",
    "    trans_in_seq = ele_in_trans_in_seq(tran_lis, feq_dict)\n",
    "    t = FPTree() #Init FPtree\n",
    "    for i in range(len(trans_in_seq)):\n",
    "        t.add_trans(trans_in_seq[i]) #Add transaction into tree\n",
    "    for ele in sorted(feq_dict.items(), key = lambda s : s[1]):\n",
    "        build_cond_tree(t.get_all_path(ele[0]))# Search for frequent itemsets, and yield the results we find.\n",
    "    result = []\n",
    "    for itemset in find_with_suffix(t, [], len(tran_lis)):\n",
    "        result.append(itemset)\n",
    "    cond = {}\n",
    "    for itemset, support in sorted(result, key=lambda i: i[0]):\n",
    "        cond[str(set(itemset))] = (set(itemset), support)\n",
    "\n",
    "    #fp_get_sup(cond)\n",
    "    ru = fp_get_rule_with_conf(cond, min_conf)\n",
    "    end = time.time()\n",
    "    f_k_t += (end-start)\n",
    "    #print(\"With min_sup: \", min_sup ,\", min_conf: \", min_conf,  \"The association rule is: \", ru)\n",
    "    print(\"FPG_kaggle_dataset_runtime: \", end-start)\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Start_IBM\")\n",
    "    start = time.time()\n",
    "    feq_dict = ele_in_seq(i_tran_lis, len(i_tran_lis), min_sup)\n",
    "    trans_in_seq = ele_in_trans_in_seq(i_tran_lis, feq_dict)\n",
    "    t = FPTree() #Init FPtree\n",
    "    for i in range(len(trans_in_seq)):\n",
    "        t.add_trans(trans_in_seq[i]) #Add transaction into tree\n",
    "    for ele in sorted(feq_dict.items(), key = lambda s : s[1]):\n",
    "        build_cond_tree(t.get_all_path(ele[0])) # Search for frequent itemsets, and yield the results we find.\n",
    "    result = []\n",
    "    for itemset in find_with_suffix(t, [], len(i_tran_lis)):\n",
    "        result.append(itemset)\n",
    "    cond = {}\n",
    "    for itemset, support in sorted(result, key=lambda i: i[0]):\n",
    "        cond[str(set(itemset))] = (set(itemset), support)\n",
    "    #fp_get_sup(cond)\n",
    "    ru = fp_get_rule_with_conf(cond, min_conf)\n",
    "    end = time.time()\n",
    "    f_i_t += (end-start)\n",
    "    #print(\"With min_sup: \", min_sup ,\", min_conf: \", min_conf,  \"The association rule is: \", ru)\n",
    "    print(\"FPG_IBM_dataset_runtime: \", end-start)\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Start_Kaggle\")\n",
    "    start = time.time()\n",
    "    freq_set= get_freq_set(tran_lis, len(tran_lis), min_sup)\n",
    "\n",
    "    #get_sup(freq_set, min_sup, len(tran_lis))\n",
    "    ru = get_rule_with_conf(freq_set, min_conf, len(tran_lis))\n",
    "    end = time.time()\n",
    "    a_k_t += (end-start)\n",
    "    #print(\"With min_sup: \", min_sup ,\", min_conf: \", min_conf,  \"The association rule is: \", ru)\n",
    "    print(\"Apriori_kaggle_dataset_runtime: \", end-start)\n",
    "\n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "    print(\"Start_IBM\")\n",
    "    start = time.time()\n",
    "    freq_set= get_freq_set(i_tran_lis, len(i_tran_lis), min_sup)\n",
    "    #get_sup(freq_set, min_sup, len(i_tran_lis))\n",
    "    ru = get_rule_with_conf(freq_set, min_conf, len(i_tran_lis))\n",
    "    end = time.time()\n",
    "    a_i_t += (end-start)\n",
    "    #print(\"With min_sup: \", min_sup ,\", min_conf: \", min_conf,  \"The association rule is: \", ru)\n",
    "    print(\"Apriori_IBM_dataset_runtime: \", end-start)\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------\")\n",
    "    time.sleep(1.2)\n",
    "print(\"fk\", float(f_k_t/10), \"fi\",float(f_i_t/10), \"ak\",float(a_k_t/10),  \"ai\", float(a_i_t/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
